sudo gluster volume status wol_grupa4 detail
Status of volume: wol_grupa4
------------------------------------------------------------------------------
Brick                : Brick ClusterMachine1.ex1.gr4:/bricks/gluster_brick/brick1
TCP Port             : 49152               
RDMA Port            : 0                   
Online               : Y                   
Pid                  : 6458                
File System          : xfs                 
Device               : /dev/sdb1           
Mount Options        : rw,seclabel,relatime,attr2,inode64,noquota
Inode Size           : 512                 
Disk Space Free      : 2.0GB               
Total Disk Space     : 2.0GB               
Inode Count          : 1048576             
Free Inodes          : 1048559             
------------------------------------------------------------------------------
Brick                : Brick ClusterMachine2.ex1.gr4:/bricks/gluster_brick/brick1
TCP Port             : 49152               
RDMA Port            : 0                   
Online               : Y                   
Pid                  : 6445                
File System          : xfs                 
Device               : /dev/sdb1           
Mount Options        : rw,seclabel,relatime,attr2,inode64,noquota
Inode Size           : 512                 
Disk Space Free      : 2.0GB               
Total Disk Space     : 2.0GB               
Inode Count          : 1048576             
Free Inodes          : 1048559             
------------------------------------------------------------------------------
Brick                : Brick ClusterMachine3.ex1.gr4:/bricks/gluster_brick/brick1
TCP Port             : 49152               
RDMA Port            : 0                   
Online               : Y                   
Pid                  : 6416                
File System          : xfs                 
Device               : /dev/sdb1           
Mount Options        : rw,seclabel,relatime,attr2,inode64,noquota
Inode Size           : 512                 
Disk Space Free      : 2.0GB               
Total Disk Space     : 2.0GB               
Inode Count          : 1048576             
Free Inodes          : 1048559             
------------------------------------------------------------------------------
Brick                : Brick ClusterMachine4.ex1.gr4:/bricks/gluster_brick/brick1
TCP Port             : 49152               
RDMA Port            : 0                   
Online               : Y                   
Pid                  : 6394                
File System          : xfs                 
Device               : /dev/sdb1           
Mount Options        : rw,seclabel,relatime,attr2,inode64,noquota
Inode Size           : 512                 
Disk Space Free      : 2.0GB               
Total Disk Space     : 2.0GB               
Inode Count          : 1048576             
Free Inodes          : 1048559             
 
sudo gluster volume status
Status of volume: wol_grupa4
Gluster process                             TCP Port  RDMA Port  Online  Pid
------------------------------------------------------------------------------
Brick ClusterMachine1.ex1.gr4:/bricks/glust
er_brick/brick1                             49152     0          Y       6458 
Brick ClusterMachine2.ex1.gr4:/bricks/glust
er_brick/brick1                             49152     0          Y       6445 
Brick ClusterMachine3.ex1.gr4:/bricks/glust
er_brick/brick1                             49152     0          Y       6416 
Brick ClusterMachine4.ex1.gr4:/bricks/glust
er_brick/brick1                             49152     0          Y       6394 
Self-heal Daemon on localhost               N/A       N/A        Y       6415 
Self-heal Daemon on ClusterMachine1.ex1.gr4 N/A       N/A        Y       6479 
Self-heal Daemon on ClusterMachine3.ex1.gr4 N/A       N/A        Y       6437 
Self-heal Daemon on ClusterMachine2.ex1.gr4 N/A       N/A        Y       6466 
 
Task Status of Volume wol_grupa4
------------------------------------------------------------------------------
There are no active volume tasks
 
sudo gluster volume info
 
Volume Name: wol_grupa4
Type: Replicate
Volume ID: 731f1f38-44da-4491-9f8a-f0fefc1efb80
Status: Started
Snapshot Count: 0
Number of Bricks: 1 x 4 = 4
Transport-type: tcp
Bricks:
Brick1: ClusterMachine1.ex1.gr4:/bricks/gluster_brick/brick1
Brick2: ClusterMachine2.ex1.gr4:/bricks/gluster_brick/brick1
Brick3: ClusterMachine3.ex1.gr4:/bricks/gluster_brick/brick1
Brick4: ClusterMachine4.ex1.gr4:/bricks/gluster_brick/brick1
Options Reconfigured:
transport.address-family: inet
nfs.disable: on
performance.client-io-threads: off
sudo gluster volume status
Status of volume: wol_grupa4
Gluster process                             TCP Port  RDMA Port  Online  Pid
------------------------------------------------------------------------------
Brick ClusterMachine1.ex1.gr4:/bricks/glust
er_brick/brick1                             49152     0          Y       6458 
Brick ClusterMachine2.ex1.gr4:/bricks/glust
er_brick/brick1                             49152     0          Y       6445 
Brick ClusterMachine3.ex1.gr4:/bricks/glust
er_brick/brick1                             49152     0          Y       6416 
Brick ClusterMachine4.ex1.gr4:/bricks/glust
er_brick/brick1                             49152     0          Y       2647 
Self-heal Daemon on localhost               N/A       N/A        Y       2657 
Self-heal Daemon on ClusterMachine2.ex1.gr4 N/A       N/A        Y       6466 
Self-heal Daemon on ClusterMachine3.ex1.gr4 N/A       N/A        Y       6437 
Self-heal Daemon on ClusterMachine1.ex1.gr4 N/A       N/A        Y       6479 
 
Task Status of Volume wol_grupa4
------------------------------------------------------------------------------
There are no active volume tasks
 
sudo gluster volume info
 
Volume Name: wol_grupa4
Type: Replicate
Volume ID: 731f1f38-44da-4491-9f8a-f0fefc1efb80
Status: Started
Snapshot Count: 0
Number of Bricks: 1 x 4 = 4
Transport-type: tcp
Bricks:
Brick1: ClusterMachine1.ex1.gr4:/bricks/gluster_brick/brick1
Brick2: ClusterMachine2.ex1.gr4:/bricks/gluster_brick/brick1
Brick3: ClusterMachine3.ex1.gr4:/bricks/gluster_brick/brick1
Brick4: ClusterMachine4.ex1.gr4:/bricks/gluster_brick/brick1
Options Reconfigured:
transport.address-family: inet
nfs.disable: on
performance.client-io-threads: off
sudo gluster pool list
UUID					Hostname               	State
702578da-8c12-4e99-95b0-dd6127a91b2e	ClusterMachine3.ex1.gr4	Connected 
869a7715-2cb1-4109-bdb7-e2f8682d1752	ClusterMachine2.ex1.gr4	Connected 
97786405-cde7-443e-905c-2287ca259ef8	ClusterMachine1.ex1.gr4	Connected 
3bf7ea61-727f-47fb-9204-0112079992ea	localhost              	Connected 
sudo gluster volume status
Status of volume: wol_grupa4
Gluster process                             TCP Port  RDMA Port  Online  Pid
------------------------------------------------------------------------------
Brick ClusterMachine1.ex1.gr4:/bricks/glust
er_brick/brick1                             49152     0          Y       6458 
Brick ClusterMachine2.ex1.gr4:/bricks/glust
er_brick/brick1                             49152     0          Y       6445 
Brick ClusterMachine3.ex1.gr4:/bricks/glust
er_brick/brick1                             49152     0          Y       6416 
Brick ClusterMachine4.ex1.gr4:/bricks/glust
er_brick/brick1                             49152     0          Y       2647 
Self-heal Daemon on localhost               N/A       N/A        Y       2657 
Self-heal Daemon on ClusterMachine3.ex1.gr4 N/A       N/A        Y       6437 
Self-heal Daemon on ClusterMachine2.ex1.gr4 N/A       N/A        Y       6466 
Self-heal Daemon on ClusterMachine1.ex1.gr4 N/A       N/A        Y       6479 
 
Task Status of Volume wol_grupa4
------------------------------------------------------------------------------
There are no active volume tasks
 
sudo gluster volume info
 
Volume Name: wol_grupa4
Type: Replicate
Volume ID: 731f1f38-44da-4491-9f8a-f0fefc1efb80
Status: Started
Snapshot Count: 0
Number of Bricks: 1 x 4 = 4
Transport-type: tcp
Bricks:
Brick1: ClusterMachine1.ex1.gr4:/bricks/gluster_brick/brick1
Brick2: ClusterMachine2.ex1.gr4:/bricks/gluster_brick/brick1
Brick3: ClusterMachine3.ex1.gr4:/bricks/gluster_brick/brick1
Brick4: ClusterMachine4.ex1.gr4:/bricks/gluster_brick/brick1
Options Reconfigured:
transport.address-family: inet
nfs.disable: on
performance.client-io-threads: off
sudo gluster pool list
UUID					Hostname               	State
702578da-8c12-4e99-95b0-dd6127a91b2e	ClusterMachine3.ex1.gr4	Connected 
869a7715-2cb1-4109-bdb7-e2f8682d1752	ClusterMachine2.ex1.gr4	Connected 
97786405-cde7-443e-905c-2287ca259ef8	ClusterMachine1.ex1.gr4	Connected 
3bf7ea61-727f-47fb-9204-0112079992ea	localhost              	Connected 
sudo gluster volume status wol_grupa4 detail
sudo gluster volume status
sudo gluster volume info
sudo gluster pool list
UUID					Hostname 	State
b77e91ca-c20b-46e1-9503-d67d3c0d0394	localhost	Connected 
sudo gluster volume status wol_grupa4 detail
sudo gluster volume status
sudo gluster volume info
sudo gluster pool list
UUID					Hostname 	State
8b7d4752-6300-4adc-b9d0-c7e2f5357e52	localhost	Connected 
sudo gluster volume status wol_grupa4 detail
sudo gluster volume status
sudo gluster volume info
sudo gluster pool list
UUID					Hostname 	State
a627712d-65b1-49e1-840f-dd71aedf55a9	localhost	Connected 
sudo gluster volume status wol_grupa4 detail
Status of volume: wol_grupa4
------------------------------------------------------------------------------
Brick                : Brick ClusterMachine1.ex1.gr4:/bricks/gluster_brick/brick1
TCP Port             : 49152               
RDMA Port            : 0                   
Online               : Y                   
Pid                  : 6433                
File System          : xfs                 
Device               : /dev/sdb1           
Mount Options        : rw,seclabel,relatime,attr2,inode64,noquota
Inode Size           : 512                 
Disk Space Free      : 2.0GB               
Total Disk Space     : 2.0GB               
Inode Count          : 1048576             
Free Inodes          : 1048559             
------------------------------------------------------------------------------
Brick                : Brick ClusterMachine2.ex1.gr4:/bricks/gluster_brick/brick1
TCP Port             : 49152               
RDMA Port            : 0                   
Online               : Y                   
Pid                  : 6392                
File System          : xfs                 
Device               : /dev/sdb1           
Mount Options        : rw,seclabel,relatime,attr2,inode64,noquota
Inode Size           : 512                 
Disk Space Free      : 2.0GB               
Total Disk Space     : 2.0GB               
Inode Count          : 1048576             
Free Inodes          : 1048559             
------------------------------------------------------------------------------
Brick                : Brick ClusterMachine3.ex1.gr4:/bricks/gluster_brick/brick1
TCP Port             : 49152               
RDMA Port            : 0                   
Online               : Y                   
Pid                  : 6387                
File System          : xfs                 
Device               : /dev/sdb1           
Mount Options        : rw,seclabel,relatime,attr2,inode64,noquota
Inode Size           : 512                 
Disk Space Free      : 2.0GB               
Total Disk Space     : 2.0GB               
Inode Count          : 1048576             
Free Inodes          : 1048559             
------------------------------------------------------------------------------
Brick                : Brick ClusterMachine4.ex1.gr4:/bricks/gluster_brick/brick1
TCP Port             : 49152               
RDMA Port            : 0                   
Online               : Y                   
Pid                  : 6356                
File System          : xfs                 
Device               : /dev/sdb1           
Mount Options        : rw,seclabel,relatime,attr2,inode64,noquota
Inode Size           : 512                 
Disk Space Free      : 2.0GB               
Total Disk Space     : 2.0GB               
Inode Count          : 1048576             
Free Inodes          : 1048559             
 
sudo gluster volume status
Status of volume: wol_grupa4
Gluster process                             TCP Port  RDMA Port  Online  Pid
------------------------------------------------------------------------------
Brick ClusterMachine1.ex1.gr4:/bricks/glust
er_brick/brick1                             49152     0          Y       6433 
Brick ClusterMachine2.ex1.gr4:/bricks/glust
er_brick/brick1                             49152     0          Y       6392 
Brick ClusterMachine3.ex1.gr4:/bricks/glust
er_brick/brick1                             49152     0          Y       6387 
Brick ClusterMachine4.ex1.gr4:/bricks/glust
er_brick/brick1                             49152     0          Y       6356 
Self-heal Daemon on localhost               N/A       N/A        Y       6377 
Self-heal Daemon on ClusterMachine3.ex1.gr4 N/A       N/A        Y       6408 
Self-heal Daemon on ClusterMachine2.ex1.gr4 N/A       N/A        Y       6413 
Self-heal Daemon on ClusterMachine1.ex1.gr4 N/A       N/A        Y       6454 
 
Task Status of Volume wol_grupa4
------------------------------------------------------------------------------
There are no active volume tasks
 
sudo gluster volume info
 
Volume Name: wol_grupa4
Type: Replicate
Volume ID: b656b884-4f20-455d-a485-bf195cbc8964
Status: Started
Snapshot Count: 0
Number of Bricks: 1 x 4 = 4
Transport-type: tcp
Bricks:
Brick1: ClusterMachine1.ex1.gr4:/bricks/gluster_brick/brick1
Brick2: ClusterMachine2.ex1.gr4:/bricks/gluster_brick/brick1
Brick3: ClusterMachine3.ex1.gr4:/bricks/gluster_brick/brick1
Brick4: ClusterMachine4.ex1.gr4:/bricks/gluster_brick/brick1
Options Reconfigured:
transport.address-family: inet
nfs.disable: on
performance.client-io-threads: off
sudo gluster pool list
UUID					Hostname               	State
a627712d-65b1-49e1-840f-dd71aedf55a9	ClusterMachine3.ex1.gr4	Connected 
8b7d4752-6300-4adc-b9d0-c7e2f5357e52	ClusterMachine2.ex1.gr4	Connected 
b77e91ca-c20b-46e1-9503-d67d3c0d0394	ClusterMachine1.ex1.gr4	Connected 
468c9ed3-cbe0-475e-bcad-dea498e649d7	localhost              	Connected 
sudo gluster volume status
Status of volume: wol_grupa4
Gluster process                             TCP Port  RDMA Port  Online  Pid
------------------------------------------------------------------------------
Brick ClusterMachine1.ex1.gr4:/bricks/glust
er_brick/brick1                             49152     0          Y       2752 
Brick ClusterMachine2.ex1.gr4:/bricks/glust
er_brick/brick1                             49152     0          Y       2645 
Brick ClusterMachine3.ex1.gr4:/bricks/glust
er_brick/brick1                             49152     0          Y       6387 
Brick ClusterMachine4.ex1.gr4:/bricks/glust
er_brick/brick1                             49152     0          Y       2649 
Self-heal Daemon on localhost               N/A       N/A        Y       2661 
Self-heal Daemon on ClusterMachine3.ex1.gr4 N/A       N/A        Y       6408 
Self-heal Daemon on ClusterMachine1.ex1.gr4 N/A       N/A        Y       2763 
Self-heal Daemon on ClusterMachine2.ex1.gr4 N/A       N/A        Y       2653 
 
Task Status of Volume wol_grupa4
------------------------------------------------------------------------------
There are no active volume tasks
 
sudo gluster volume info
 
Volume Name: wol_grupa4
Type: Replicate
Volume ID: b656b884-4f20-455d-a485-bf195cbc8964
Status: Started
Snapshot Count: 0
Number of Bricks: 1 x 4 = 4
Transport-type: tcp
Bricks:
Brick1: ClusterMachine1.ex1.gr4:/bricks/gluster_brick/brick1
Brick2: ClusterMachine2.ex1.gr4:/bricks/gluster_brick/brick1
Brick3: ClusterMachine3.ex1.gr4:/bricks/gluster_brick/brick1
Brick4: ClusterMachine4.ex1.gr4:/bricks/gluster_brick/brick1
Options Reconfigured:
performance.client-io-threads: off
nfs.disable: on
transport.address-family: inet
sudo gluster pool list
UUID					Hostname               	State
a627712d-65b1-49e1-840f-dd71aedf55a9	ClusterMachine3.ex1.gr4	Connected 
8b7d4752-6300-4adc-b9d0-c7e2f5357e52	ClusterMachine2.ex1.gr4	Connected 
b77e91ca-c20b-46e1-9503-d67d3c0d0394	ClusterMachine1.ex1.gr4	Connected 
468c9ed3-cbe0-475e-bcad-dea498e649d7	localhost              	Connected 
